{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "labels *= 100\n",
    "\n",
    "# train-test split\n",
    "com_train_features, test_features, com_train_labels, test_labels = train_test_split(\n",
    "features, labels, random_state=42)\n",
    "\n",
    "# train -- > train + dev split\n",
    "train_features, dev_features, train_labels, dev_labels = train_test_split(\n",
    "com_train_features, com_train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Different Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor(estimator, X_train, y_train, cv, name):\n",
    "    cv_results = cross_validate(estimator,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                cv=cv,\n",
    "                                scoring=\"neg_mean_absolute_error\",\n",
    "                                return_train_score=True,\n",
    "                                return_estimator=True)\n",
    "\n",
    "    cv_train_error = -1* cv_results['train_score']\n",
    "    cv_test_error = -1 * cv_results['test_score' ]\n",
    "\n",
    "    print(f\"On an average, {name} makes an error of \"\n",
    "    f\"{cv_train_error.mean():.3f}k +/- {cv_train_error.std():.3f}k on the training set.\")\n",
    "    print(f\"On an average, {name} makes an error of \"\n",
    "    f\"{cv_test_error.mean():3f}k +/- {cv_test_error.std():.3f}k on the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On an average, GB makes an error of 35.394k +/- 0.273k on the training set.\n",
      "On an average, GB makes an error of 36.774176k +/- 0.721k on the test set.\n"
     ]
    }
   ],
   "source": [
    "train_regressor(GradientBoostingRegressor(), com_train_features, com_train_labels, cv, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme gradient boosting (XGBoost) is the latest boosting technique. It is more regularized form of gradient boosting. With regularization, it is\n",
    "able to achieve better generalization performance than gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_regressor = XGBRegressor (objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On an average, XGBoostRegressor makes an error of 17.660k +/- 0.246k on the training set.\n",
      "On an average, XGBoostRegressor makes an error of 31.340163k +/- 0.791k on the test set.\n"
     ]
    }
   ],
   "source": [
    "train_regressor(\n",
    "    xgb_regressor, com_train_features, com_train_labels, cv, \"XGBoostRegressor\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
